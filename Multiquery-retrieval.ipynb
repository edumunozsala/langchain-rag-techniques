{"cells":[{"cell_type":"markdown","metadata":{"id":"gJq7RFOw3ULM"},"source":["# Multiquery Retrieval"]},{"cell_type":"markdown","metadata":{},"source":["The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/edumunozsala/langchain-rag-techniques/blob/main/Multiquery-retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9326,"status":"ok","timestamp":1696136091544,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"RRYSu48huSUW","outputId":"af7ffc6c-c37f-429a-f2a3-953d3b27ddc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install langchain openai tiktoken faiss-cpu"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4686,"status":"ok","timestamp":1696135937837,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"J-KFB7J_u_3L","outputId":"27b3f544-bae7-4c02-99ec-b807c5d71a4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: langchain\n","Version: 0.0.305\n","Summary: Building applications with LLMs through composability\n","Home-page: https://github.com/langchain-ai/langchain\n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n"]}],"source":["!pip show langchain"]},{"cell_type":"markdown","metadata":{},"source":["### Load the API Key"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","\n","# Load the enviroment variables\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{},"source":["### Load the PDF document"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# Load a PDF file, extract text into documents and create a FAISS vectorstore with Langchain\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.schema import Document\n","\n","import os\n","import re "]},{"cell_type":"markdown","metadata":{},"source":["Helper functions to load the PDF file, split it and create the Documents"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","def split_text_documents(text, source=\"Not provided\", chunk_size=1000, chunk_overlap=0):\n","    \"\"\"\n","    Split the documents in the reader into smaller chuncks.\n","    \n","    Args:\n","    reader (PdfReader): The PdfReader object to be splitted.\n","    Returns:\n","    str: The summarized document.\n","    \"\"\"\n","    # Create a text splitter\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\" \", \",\", \"\\n\"]\n","    )\n","    #Split the text\n","    texts = text_splitter.split_text(text)\n","    # Create a list of documents\n","    docs = [Document(page_content=t, metadata={\"source\":source, \"chunk\":i}) for i,t in enumerate(texts)]\n","    \"\"\"\n","    documents = text_splitter.split_documents(documents=reader)\n","    print(f\"Splitted into {len(documents)} chunks\")\n","    # Update the metadata with the source url\n","    for doc in documents:\n","        #old_path = doc.metadata[\"source\"]\n","        #new_url = old_path.replace(\"langchain-docs\", \"https:/\")\n","        #doc.metadata.update({\"source\": new_url})\n","        print(doc.metadata)\n","    \"\"\"\n","    \n","    return docs\n","\n","def load_pdf_from_url(path, files):\n","    \"\"\"\n","    Load one or more PDF documents from the directory in the parameter path.\n","\n","    Args:\n","    path: directory where the file or files are located\n","    files: list of file names\n","\n","    Returns:\n","    str: the loaded document\n","    \"\"\"\n","    \n","    # creating a pdf reader object \n","    if path=='' or files=='':\n","        print('Error: file not found')\n","        return None\n","    else:\n","        reader = PyPDFLoader(os.path.join(path, files[0])) # 'data/Retrieve rerank generate.pdf') \n","    \n","    # printing number of pages in pdf file \n","    # print(len(reader.pages)) \n","    return reader.load()\n","\n","def extract_text_from_pdf(path, file):\n","    \"\"\"\n","    Extract and return the text inside a PDF documents in the directory in the parameter path.\n","\n","    Args:\n","    path: directory where the file or files are located\n","    files: list of file names\n","\n","    Returns:\n","    str: the the text in the PDF file\n","    \"\"\"\n","    files=[file]\n","    pages = load_pdf_from_url(path, files)\n","    \n","    \"Join all pages in one single text\"\n","    text=\"\"\n","    for page in pages:\n","        raw_page = re.sub('-\\s+', '', page.page_content)\n","        text += \" \".join(raw_page.split())\n","        text += \"\\n\"\n","        \n","    return text\n","\n","def load_pdf_from_file(path, file, chunk_size, chunk_overlap):\n","    \"\"\"\n","    Load the PDF document file from the directory in the parameter path. Returns a list of Langchain Documents\n","\n","    Args:\n","    path: directory where the file or files are located\n","    file: file name\n","\n","    Returns:\n","    lst: list of documents\n","    \"\"\"\n","    \n","    # creating a pdf reader object \n","    if path=='' or file=='':\n","        print('Error: file not found')\n","        return None\n","    else:\n","        # Read and clean the text in the PDf file\n","        text= extract_text_from_pdf(path, file)\n","        # Split the text and create a list of documents\n","        documents = split_text_documents(text, source=file, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","\n","  \n","    # printing number of pages in pdf file \n","    print(len(documents)) \n","    \n","    return documents\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["42\n"]}],"source":["path=\"data\"\n","file=\"Attention is all you need.pdf\"\n","\n","docs= load_pdf_from_file(path, file, 1000, 50)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["42\n"]}],"source":["print(len(docs))"]},{"cell_type":"markdown","metadata":{},"source":["### Create the Vector index"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#from langchain.vectorstores import Chroma\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings.openai import OpenAIEmbeddings"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["embedding = OpenAIEmbeddings()\n","faiss_vectorstore = FAISS.from_documents(docs, embedding)\n","\n","# Create a retriever from the vectorstore\n","#faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 3})"]},{"cell_type":"markdown","metadata":{},"source":["## Create the Multiquery Retriever"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"OB3IcjGDi6iF"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.retrievers.multi_query import MultiQueryRetriever"]},{"cell_type":"markdown","metadata":{},"source":["**Simple usage**\n","\n","Specify the LLM to use for query generation, and the retriever will do the rest."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9-wZ1BTOa5fX"},"outputs":[],"source":["# Define the LLM\n","llm = ChatOpenAI(temperature=0)\n","# Set the multiquery retriever\n","retriever_from_llm = MultiQueryRetriever.from_llm(\n","    retriever=faiss_vectorstore.as_retriever(search_kwargs={\"k\": 3}), llm=llm, include_original=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Set the logging properties"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Set logging for the queries\n","import logging\n","\n","logging.basicConfig()\n","logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"]},{"cell_type":"markdown","metadata":{},"source":["We test the retriever"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":536,"status":"ok","timestamp":1696139615650,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"xLyCs04RlNZK","outputId":"0c85926e-adf9-47fb-b505-ddede6e09ff6"},"outputs":[{"data":{"text/plain":["4"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["question=\"How are transformers related to convolutional neural networks?\"\n","\n","unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n","len(unique_docs)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1696139640083,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"tOL_2o2tu3rm","outputId":"a57d5ac9-f1d5-41ce-a828-d241ab339210"},"outputs":[{"data":{"text/plain":["[Document(page_content='of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block,', metadata={'source': 'Attention is all you need.pdf', 'chunk': 4}),\n"," Document(page_content='neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning', metadata={'source': 'Attention is all you need.pdf', 'chunk': 5}),\n"," Document(page_content='Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model', metadata={'source': 'Attention is all you need.pdf', 'chunk': 0}),\n"," Document(page_content='feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0,xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we', metadata={'source': 'Attention is all you need.pdf', 'chunk': 14})]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["unique_docs"]},{"cell_type":"markdown","metadata":{},"source":["## Build the RAG Chain"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Em1okC6azwOM"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableLambda, RunnablePassthrough"]},{"cell_type":"markdown","metadata":{},"source":["Define the Prompt template"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Create the Template \n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n"]},{"cell_type":"markdown","metadata":{},"source":["Create the chain"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["model = ChatOpenAI(temperature=0)\n","\n","chain = (\n","    {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Invoke the chain"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["'Transformers are related to convolutional neural networks in that both models are used for sequence transduction tasks. However, transformers differ from convolutional neural networks in that they do not rely on recurrence or convolutions. Instead, transformers use attention mechanisms to draw global dependencies between input and output, allowing for more parallelization and reducing the number of operations required to relate signals from different positions in the sequence.'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke(\"How are transformers related to convolutional neural networks?\")"]},{"cell_type":"markdown","metadata":{},"source":["## Create a custom template"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from typing import List\n","\n","from langchain.chains import LLMChain\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain.prompts import PromptTemplate\n","from pydantic import BaseModel, Field\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.retrievers.multi_query import MultiQueryRetriever"]},{"cell_type":"markdown","metadata":{},"source":["Define a custom prompt for multiquery, asking the model for creating 5 questions. We also create an output parses to get the questions from the LLM."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Output parser will split the LLM result into a list of queries\n","class LineList(BaseModel):\n","    # \"lines\" is the key (attribute name) of the parsed output\n","    lines: List[str] = Field(description=\"Lines of text\")\n","\n","\n","class LineListOutputParser(PydanticOutputParser):\n","    def __init__(self) -> None:\n","        super().__init__(pydantic_object=LineList)\n","\n","    def parse(self, text: str) -> LineList:\n","        lines = text.strip().split(\"\\n\")\n","        return LineList(lines=lines)\n","\n","\n","output_parser = LineListOutputParser()\n","\n","QUERY_PROMPT = PromptTemplate(\n","    input_variables=[\"question\"],\n","    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n","    different versions of the given user question to retrieve relevant documents from a vector \n","    database. By generating multiple perspectives on the user question, your goal is to help\n","    the user overcome some of the limitations of the distance-based similarity search. \n","    Provide these alternative questions separated by newlines.\n","    Original question: {question}\"\"\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Set the logging to get the multiqueries"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Set logging for the queries\n","import logging\n","\n","logging.basicConfig()\n","logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"]},{"cell_type":"markdown","metadata":{},"source":["Create a LLM Chain to extract the multiple queries from our original query in an appropiate format"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["llm = ChatOpenAI(temperature=0)\n","\n","# Chain\n","llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n","\n","# Run\n","retriever_custom_multi = MultiQueryRetriever(\n","    retriever=faiss_vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",")  # \"lines\" is the key (attribute name) of the parsed output\n"]},{"cell_type":"markdown","metadata":{},"source":["Invoke the retriever to test it"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the relationship between transformers and convolutional neural networks?', '2. Can you explain how transformers and convolutional neural networks are connected?', '3. In what way are transformers and convolutional neural networks related?', '4. What is the connection between transformers and convolutional neural networks?', '5. How do transformers and convolutional neural networks relate to each other?']\n"]}],"source":["# Results\n","unique_docs = retriever_custom_multi.get_relevant_documents(\n","    query=\"How are transformers related to convolutional neural networks?\"\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block,', metadata={'source': 'Attention is all you need.pdf', 'chunk': 4}),\n"," Document(page_content='neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning', metadata={'source': 'Attention is all you need.pdf', 'chunk': 5}),\n"," Document(page_content='Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model', metadata={'source': 'Attention is all you need.pdf', 'chunk': 0}),\n"," Document(page_content='we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate', metadata={'source': 'Attention is all you need.pdf', 'chunk': 30}),\n"," Document(page_content='feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0,xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we', metadata={'source': 'Attention is all you need.pdf', 'chunk': 14}),\n"," Document(page_content='seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15', metadata={'source': 'Attention is all you need.pdf', 'chunk': 41})]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["unique_docs"]},{"cell_type":"markdown","metadata":{},"source":["## Build the RAG Chain"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableLambda, RunnablePassthrough"]},{"cell_type":"markdown","metadata":{},"source":["Define the Prompt template"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Create the Template \n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","model = ChatOpenAI(temperature=0)\n","\n","chain = (\n","    {\"context\": retriever_custom_multi, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Invoke the chain"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the relationship between transformers and convolutional neural networks?', '2. Can you explain how transformers and convolutional neural networks are connected?', '3. In what way are transformers and convolutional neural networks related?', '4. What is the connection between transformers and convolutional neural networks?', '5. How do transformers and convolutional neural networks relate to each other?']\n"]},{"data":{"text/plain":["'Transformers are related to convolutional neural networks in that they both aim to reduce sequential computation. However, transformers rely entirely on attention mechanisms to draw global dependencies between input and output, while convolutional neural networks use convolutional layers as their basic building block and require a varying number of operations to relate signals from different input or output positions based on their distance.'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke(\"How are transformers related to convolutional neural networks?\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1lsT1V_U1Gq-jv09wv0ok5QHdyRjJyNxm","timestamp":1703241353984},{"file_id":"1llBvUXvy9sgvfOX-IaiAAHG9cZm7UC0A","timestamp":1696597847452},{"file_id":"1YXXWaUSFR_Km1NVL2best5hkPkh88Rhg","timestamp":1696115734680},{"file_id":"1J1FOyWE2dv9Pa_flwBdSsS2f7Ly6yllt","timestamp":1695946769549},{"file_id":"1sAmHXUlZNJg5ibJdTq7kQ1QLoR6Fy9fN","timestamp":1695946575577},{"file_id":"1_57iwYV205vVWrmJnfTUjWIo5XB3e-5g","timestamp":1695733772895},{"file_id":"1BT_kRFMP27lmwAoWeIhiie0VPs0oqShz","timestamp":1695731972211}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
