{"cells":[{"cell_type":"markdown","metadata":{"id":"gJq7RFOw3ULM"},"source":["# Parent Document Retriever"]},{"cell_type":"markdown","metadata":{},"source":["When splitting documents for retrieval, there are often conflicting desires:\n","\n","- You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n","- You want to have long enough documents that the context of each chunk is retained.\n","\n","**Context Enrichment**\n","\n","The concept here is to retrieve smaller chunks for better search quality, but add up surrounding context for LLM to reason upon.\n","There are two options — to expand context by sentences around the smaller retrieved chunk or to split documents recursively into a number of larger parent chunks, containing smaller child chunks.\n","\n","The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents."]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/edumunozsala/langchain-rag-techniques/blob/main/parent-document-retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9326,"status":"ok","timestamp":1696136091544,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"RRYSu48huSUW","outputId":"af7ffc6c-c37f-429a-f2a3-953d3b27ddc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install langchain openai tiktoken pinecone-client"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4686,"status":"ok","timestamp":1696135937837,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"J-KFB7J_u_3L","outputId":"27b3f544-bae7-4c02-99ec-b807c5d71a4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: langchain\n","Version: 0.0.305\n","Summary: Building applications with LLMs through composability\n","Home-page: https://github.com/langchain-ai/langchain\n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n"]}],"source":["!pip show langchain"]},{"cell_type":"markdown","metadata":{},"source":["### Load the API Key"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","\n","# Load the enviroment variables\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{},"source":["### Load the PDF document"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# Load a PDF file, extract text into documents and create a FAISS vectorstore with Langchain\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.schema import Document\n","\n","import os\n","import re "]},{"cell_type":"markdown","metadata":{},"source":["Helper functions to load the PDF file, split it and create the Documents"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","def split_text_documents(text, source=\"Not provided\", chunk_size=1000, chunk_overlap=0):\n","    \"\"\"\n","    Split the documents in the reader into smaller chuncks.\n","    \n","    Args:\n","    reader (PdfReader): The PdfReader object to be splitted.\n","    Returns:\n","    str: The summarized document.\n","    \"\"\"\n","    # Create a text splitter\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\" \", \",\", \"\\n\"]\n","    )\n","    #Split the text\n","    texts = text_splitter.split_text(text)\n","    # Create a list of documents\n","    docs = [Document(page_content=t, metadata={\"source\":source, \"chunk\":i}) for i,t in enumerate(texts)]\n","    \"\"\"\n","    documents = text_splitter.split_documents(documents=reader)\n","    print(f\"Splitted into {len(documents)} chunks\")\n","    # Update the metadata with the source url\n","    for doc in documents:\n","        #old_path = doc.metadata[\"source\"]\n","        #new_url = old_path.replace(\"langchain-docs\", \"https:/\")\n","        #doc.metadata.update({\"source\": new_url})\n","        print(doc.metadata)\n","    \"\"\"\n","    \n","    return docs\n","\n","def load_pdf_from_url(path, files):\n","    \"\"\"\n","    Load one or more PDF documents from the directory in the parameter path.\n","\n","    Args:\n","    path: directory where the file or files are located\n","    files: list of file names\n","\n","    Returns:\n","    str: the loaded document\n","    \"\"\"\n","    \n","    # creating a pdf reader object \n","    if path=='' or files=='':\n","        print('Error: file not found')\n","        return None\n","    else:\n","        reader = PyPDFLoader(os.path.join(path, files[0])) # 'data/Retrieve rerank generate.pdf') \n","    \n","    # printing number of pages in pdf file \n","    # print(len(reader.pages)) \n","    return reader.load()\n","\n","def extract_text_from_pdf(path, file):\n","    \"\"\"\n","    Extract and return the text inside a PDF documents in the directory in the parameter path.\n","\n","    Args:\n","    path: directory where the file or files are located\n","    files: list of file names\n","\n","    Returns:\n","    str: the the text in the PDF file\n","    \"\"\"\n","    files=[file]\n","    pages = load_pdf_from_url(path, files)\n","    \n","    \"Join all pages in one single text\"\n","    text=\"\"\n","    for page in pages:\n","        raw_page = re.sub('-\\s+', '', page.page_content)\n","        text += \" \".join(raw_page.split())\n","        text += \"\\n\"\n","        \n","    return text\n","\n","def load_pdf_from_file(path, file, chunk_size, chunk_overlap):\n","    \"\"\"\n","    Load the PDF document file from the directory in the parameter path. Returns a list of Langchain Documents\n","\n","    Args:\n","    path: directory where the file or files are located\n","    file: file name\n","\n","    Returns:\n","    lst: list of documents\n","    \"\"\"\n","    \n","    # creating a pdf reader object \n","    if path=='' or file=='':\n","        print('Error: file not found')\n","        return None\n","    else:\n","        # Read and clean the text in the PDf file\n","        text= extract_text_from_pdf(path, file)\n","        # Split the text and create a list of documents\n","        documents = split_text_documents(text, source=file, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","\n","  \n","    # printing number of pages in pdf file \n","    print(len(documents)) \n","    \n","    return documents\n"]},{"cell_type":"markdown","metadata":{},"source":["Read the PDf documento of ther user 1"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6\n"]}],"source":["path=\"data\"\n","file=\"Attention is all you need.pdf\"\n","\n","docs1= load_pdf_from_file(path, file, 7500, 100)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6\n"]}],"source":["print(len(docs1))"]},{"cell_type":"markdown","metadata":{},"source":["## Get the index and Create the Retriever"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","from langchain.storage.file_system import LocalFileStore\n","from langchain.storage._lc_store import create_kv_docstore\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","from langchain.retrievers import ParentDocumentRetriever"]},{"cell_type":"markdown","metadata":{},"source":["We create a function where we create the vector store and build the retriever based on the Parent Document Retriever strategy."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def build_db_retriever():\n","    # Define the e beddings\n","    embeddings = OpenAIEmbeddings()\n","    # Define the parent splitter, parrent size 1,500\n","    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n","    # Define the chid splitter\n","    child_splitter = RecursiveCharacterTextSplitter(chunk_size=300)\n","    #store = InMemoryStore()\n","    fs = LocalFileStore(\"./chroma_db_filestore\")\n","    store = create_kv_docstore(fs)\n","    # Create the Vectorstore\n","    vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=embeddings,\n","                         persist_directory=\"chroma_db/\")\n","    # Define the Parent Document retriever\n","    big_chunks_retriever = ParentDocumentRetriever(\n","        vectorstore=vectorstore,\n","        docstore=store,\n","        child_splitter=child_splitter,\n","        parent_splitter=parent_splitter,\n","    )\n","    \n","    return big_chunks_retriever, vectorstore"]},{"cell_type":"markdown","metadata":{},"source":["Add the docs of user 1 into the index\n"," "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Build the vectorstore and the retriever\n","retriever, vectorstore= build_db_retriever()\n","# Load the documents\n","# Add the document\n","retriever.add_documents(docs1)\n"]},{"cell_type":"markdown","metadata":{},"source":["Let’s now call the vector store search functionality - we should see that it returns small chunks (since we’re storing the small chunks)."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["sub_docs = vectorstore.similarity_search(\"convolutional neural network\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary\n"]}],"source":["print(sub_docs[0].page_content)"]},{"cell_type":"markdown","metadata":{},"source":["We can also test the retriever, that returns the parent document"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Get the docs\n","retrieved_docs= retriever.get_relevant_documents(\"convolutional neural network\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["1492"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(retrieved_docs[0].page_content)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent\n"]}],"source":["print(retrieved_docs[0].page_content)"]},{"cell_type":"markdown","metadata":{},"source":["## Recreate the DB and the Retriever"]},{"cell_type":"markdown","metadata":{},"source":["When your vector store is already created and saved, you can recreate the retriever without needing to split and reload the documents "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Recreate the vectorstore and the retriever\n","retriever, vectorstore= build_db_retriever()"]},{"cell_type":"markdown","metadata":{},"source":["Test the retriever"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1492\n","and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent\n"]}],"source":["# Get the docs\n","retrieved_docs= retriever.get_relevant_documents(\"convolutional neural network\")\n","print(len(retrieved_docs[0].page_content))\n","print(retrieved_docs[0].page_content)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Build the RAG Chain"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Em1okC6azwOM"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import (\n","    ConfigurableField,\n","    RunnableBinding,\n","    RunnableLambda,\n","    RunnablePassthrough,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define the Prompt template"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Create the Template \n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","# Create the model for LLM Chain\n","model = ChatOpenAI(temperature=0)\n"]},{"cell_type":"markdown","metadata":{},"source":["We can now create the chain using our parent document retriever"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Invoke the chain to answer a question from user 1"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["'Based on the given context, it is mentioned that the Transformer architecture is based solely on attention mechanisms and does not use convolutional neural networks. Therefore, transformers are not directly related to convolutional neural networks.'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke(\n","    \"How are transformers related to convolutional neural networks?\"\n",")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1lsT1V_U1Gq-jv09wv0ok5QHdyRjJyNxm","timestamp":1703241353984},{"file_id":"1llBvUXvy9sgvfOX-IaiAAHG9cZm7UC0A","timestamp":1696597847452},{"file_id":"1YXXWaUSFR_Km1NVL2best5hkPkh88Rhg","timestamp":1696115734680},{"file_id":"1J1FOyWE2dv9Pa_flwBdSsS2f7Ly6yllt","timestamp":1695946769549},{"file_id":"1sAmHXUlZNJg5ibJdTq7kQ1QLoR6Fy9fN","timestamp":1695946575577},{"file_id":"1_57iwYV205vVWrmJnfTUjWIo5XB3e-5g","timestamp":1695733772895},{"file_id":"1BT_kRFMP27lmwAoWeIhiie0VPs0oqShz","timestamp":1695731972211}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
