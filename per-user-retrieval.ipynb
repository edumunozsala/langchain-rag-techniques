{"cells":[{"cell_type":"markdown","metadata":{"id":"gJq7RFOw3ULM"},"source":["# Per User Retrieval"]},{"cell_type":"markdown","metadata":{},"source":["When building a retrieval app, you often have to build it with multiple users in mind. This means that you may be storing data not just for one user, but for many different users, and they should not be able to see eachother’s data. This means that you need to be able to configure your retrieval chain to only retrieve certain information.\n","\n","This generally involves two steps.\n","\n","- Step 1: Make sure the retriever you are using supports multiple users, for example, each vectorstore and retriever may have their own mechanism, and may be called different things (namespaces, multi-tenancy, etc). For vectorstores, this is generally exposed as a keyword argument that is passed in during similarity_search.\n","\n","- Step 2: Add that parameter as a configurable field for the chain. This will let you easily call the chain and configure any relevant flags at runtime.\n","\n","- Step 3: Call the chain with that configurable field"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/edumunozsala/langchain-rag-techniques/blob/main/per-user-retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9326,"status":"ok","timestamp":1696136091544,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"RRYSu48huSUW","outputId":"af7ffc6c-c37f-429a-f2a3-953d3b27ddc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install langchain openai tiktoken pinecone-client"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4686,"status":"ok","timestamp":1696135937837,"user":{"displayName":"Sam Witteveen","userId":"13451642680591748340"},"user_tz":-480},"id":"J-KFB7J_u_3L","outputId":"27b3f544-bae7-4c02-99ec-b807c5d71a4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: langchain\n","Version: 0.0.305\n","Summary: Building applications with LLMs through composability\n","Home-page: https://github.com/langchain-ai/langchain\n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n"]}],"source":["!pip show langchain"]},{"cell_type":"markdown","metadata":{},"source":["### Load the API Key"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","\n","# Load the enviroment variables\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{},"source":["### Load the PDF document"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# Load a PDF file, extract text into documents and create a FAISS vectorstore with Langchain\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.schema import Document\n","\n","import os\n","import re "]},{"cell_type":"markdown","metadata":{},"source":["Helper functions to load the PDF file, split it and create the Documents"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","def split_text_documents(text, source=\"Not provided\", chunk_size=1000, chunk_overlap=0):\n","    \"\"\"\n","    Split the documents in the reader into smaller chuncks.\n","    \n","    Args:\n","    reader (PdfReader): The PdfReader object to be splitted.\n","    Returns:\n","    str: The summarized document.\n","    \"\"\"\n","    # Create a text splitter\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\" \", \",\", \"\\n\"]\n","    )\n","    #Split the text\n","    texts = text_splitter.split_text(text)\n","    # Create a list of documents\n","    docs = [Document(page_content=t, metadata={\"source\":source, \"chunk\":i}) for i,t in enumerate(texts)]\n","    \"\"\"\n","    documents = text_splitter.split_documents(documents=reader)\n","    print(f\"Splitted into {len(documents)} chunks\")\n","    # Update the metadata with the source url\n","    for doc in documents:\n","        #old_path = doc.metadata[\"source\"]\n","        #new_url = old_path.replace(\"langchain-docs\", \"https:/\")\n","        #doc.metadata.update({\"source\": new_url})\n","        print(doc.metadata)\n","    \"\"\"\n","    \n","    return docs\n","\n","def load_pdf_from_url(path, files):\n","    \"\"\"\n","    Load one or more PDF documents from the directory in the parameter path.\n","\n","    Args:\n","    path: directory where the file or files are located\n","    files: list of file names\n","\n","    Returns:\n","    str: the loaded document\n","    \"\"\"\n","    \n","    # creating a pdf reader object \n","    if path=='' or files=='':\n","        print('Error: file not found')\n","        return None\n","    else:\n","        reader = PyPDFLoader(os.path.join(path, files[0])) # 'data/Retrieve rerank generate.pdf') \n","    \n","    # printing number of pages in pdf file \n","    # print(len(reader.pages)) \n","    return reader.load()\n","\n","def extract_text_from_pdf(path, file):\n","    \"\"\"\n","    Extract and return the text inside a PDF documents in the directory in the parameter path.\n","\n","    Args:\n","    path: directory where the file or files are located\n","    files: list of file names\n","\n","    Returns:\n","    str: the the text in the PDF file\n","    \"\"\"\n","    files=[file]\n","    pages = load_pdf_from_url(path, files)\n","    \n","    \"Join all pages in one single text\"\n","    text=\"\"\n","    for page in pages:\n","        raw_page = re.sub('-\\s+', '', page.page_content)\n","        text += \" \".join(raw_page.split())\n","        text += \"\\n\"\n","        \n","    return text\n","\n","def load_pdf_from_file(path, file, chunk_size, chunk_overlap):\n","    \"\"\"\n","    Load the PDF document file from the directory in the parameter path. Returns a list of Langchain Documents\n","\n","    Args:\n","    path: directory where the file or files are located\n","    file: file name\n","\n","    Returns:\n","    lst: list of documents\n","    \"\"\"\n","    \n","    # creating a pdf reader object \n","    if path=='' or file=='':\n","        print('Error: file not found')\n","        return None\n","    else:\n","        # Read and clean the text in the PDf file\n","        text= extract_text_from_pdf(path, file)\n","        # Split the text and create a list of documents\n","        documents = split_text_documents(text, source=file, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","\n","  \n","    # printing number of pages in pdf file \n","    print(len(documents)) \n","    \n","    return documents\n"]},{"cell_type":"markdown","metadata":{},"source":["Read the PDf documento of ther user 1"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["42\n"]}],"source":["path=\"data\"\n","file=\"Attention is all you need.pdf\"\n","\n","docs1= load_pdf_from_file(path, file, 1000, 50)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["42\n"]}],"source":["print(len(docs1))"]},{"cell_type":"markdown","metadata":{},"source":["Read the PDf documento of ther user 2"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["266\n","266\n"]}],"source":["path=\"data\"\n","file=\"llama-2.pdf\"\n","\n","docs2= load_pdf_from_file(path, file, 1000, 50)\n","print(len(docs2))"]},{"cell_type":"markdown","metadata":{},"source":["### Create the Vector index"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\edumu\\Google Drive\\Projects\\langchain-doc-loaders\\venv\\Lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["import pinecone\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Pinecone"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# initialize pinecone\n","pinecone.init(\n","    api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n","    environment=os.getenv(\"PINECONE_ENVIRONMENT_REGION\"),  # next to api key in console\n",")\n","\n","index_name = \"langchain-demo\"\n","\n","# First, check if our index already exists. If it doesn't, we create it\n","if index_name not in pinecone.list_indexes():\n","    # we create a new index\n","    pinecone.create_index(name=index_name, metric=\"cosine\", dimension=1536)\n"]},{"cell_type":"markdown","metadata":{},"source":["Add the docs of user 1 into the index\n"," "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n","embeddings = OpenAIEmbeddings()\n","# Add the documents to the index\n","docsearch = Pinecone.from_documents(docs1, embeddings, index_name=index_name, namespace=\"user 1\")\n","# Add the documents to the index\n","docsearch = Pinecone.from_documents(docs2, embeddings, index_name=index_name, namespace=\"user 2\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Get the index and Create the Retriever"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Get the index\n","index = pinecone.Index(\"langchain-demo\")\n","# Set the embeddings we'll use and was used to add the docs\n","embeddings = OpenAIEmbeddings()\n","# Create the vectorstore\n","vectorstore = Pinecone(index, embeddings, \"text\")"]},{"cell_type":"markdown","metadata":{},"source":["Now, we can create a retriever for the user 1 and test it"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning', metadata={'chunk': 5.0, 'source': 'Attention is all you need.pdf'}),\n"," Document(page_content='of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block,', metadata={'chunk': 4.0, 'source': 'Attention is all you need.pdf'}),\n"," Document(page_content='Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model', metadata={'chunk': 0.0, 'source': 'Attention is all you need.pdf'}),\n"," Document(page_content='summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence of continuous representations z= (z1,...,z n). Given z, the decoder then generates an', metadata={'chunk': 6.0, 'source': 'Attention is all you need.pdf'})]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# This will only get documents for Ankush\n","vectorstore.as_retriever(search_kwargs={\"namespace\": \"user 1\"}).get_relevant_documents(\n","    \"How are transformers related to convolutional neural networks?\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["And we can repeat the operation for user 2"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='Useinanyotherway that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2. Hardware and Software (Section 2.2) Training Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso performed on third-party cloud compute. Carbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO 2eq, 100% of which were offset by Meta’s sustainability program. Training Data (Sections 2.1 and 3) Overview Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as wellasoveronemillionnewhuman-annotatedexamples. Neitherthepretraining nor the fine-tuning datasets include Meta user data. Data Freshness The pretraining data has a cutoff of September 2022, but some tuning data is more', metadata={'chunk': 264.0, 'source': 'llama-2.pdf'}),\n"," Document(page_content='open-source models. Standard Benchmarks. In Table 20, we show results on several standard benchmarks. CodeGeneration. InTable21,wecompareresultsof Llama 2 withpopularopensourcemodelsonthe Human-Eval and MBPP code generation benchmarks. World Knowledge. We evaluate the Llama 2 model together with other open-source models on the NaturalQuestions and TriviaQA benchmarks (Table 22). ReadingComprehension InTable23 wereport zero-shotand few-shot resultsonSQUADand zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better. Exams. In Table 24, we present fine-grained results from the English part of the AGI Eval (Zhong et al., 2023) benchmark. AGI Eval is a collection of standardized exams in different subjects. 48\\nHumanities STEM Social Sciences Other Average MPT7B 26.7 25.3 27.1 28.2 26.8 30B 44.5 39.0 52.8 52.9 46.9 Falcon7B 26.4 26.2 24.7 27.4 26.2 40B 49.3 45.5 65.4 65.0', metadata={'chunk': 173.0, 'source': 'llama-2.pdf'}),\n"," Document(page_content='We are releasing the following models to the general public for research and commercial use‡: 1.Llama 2 ,anupdatedversionof Llama 1,trainedonanewmixofpubliclyavailabledata. Wealso increasedthesizeofthepretrainingcorpusby40%,doubledthecontextlengthofthemodel,and adoptedgrouped-queryattention(Ainslieetal.,2023). Wearereleasingvariantsof Llama 2 with 7B,13B,and70Bparameters. Wehavealsotrained34Bvariants,whichwereportoninthispaper but are not releasing.§ 2.Llama 2-Chat , a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. WebelievethattheopenreleaseofLLMs,whendonesafely,willbeanetbenefittosociety. LikeallLLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaimanet al.,2023). Testingconductedtodate hasbeeninEnglish andhasnot— andcouldnot— cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat ,', metadata={'chunk': 9.0, 'source': 'llama-2.pdf'}),\n"," Document(page_content='Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗ GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com †Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023\\nContents 1 Introduction 3 2 Pretraining 5 2.1 Pretraining Data . . . . . .', metadata={'chunk': 1.0, 'source': 'llama-2.pdf'})]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# This will only get documents for Ankush\n","vectorstore.as_retriever(search_kwargs={\"namespace\": \"user 2\"}).get_relevant_documents(\n","    \"What datasets have benn used to train Llama-2?\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Build the RAG Chain"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Em1okC6azwOM"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import (\n","    ConfigurableField,\n","    RunnableBinding,\n","    RunnableLambda,\n","    RunnablePassthrough,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define the Prompt template"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Create the Template \n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","# Create the model for LLM Chain\n","model = ChatOpenAI(temperature=0)\n"]},{"cell_type":"markdown","metadata":{},"source":["Create the retriever. Here we mark the retriever as having a configurable field. All vectorstore retrievers have search_kwargs as a field. This is just a dictionary, with vectorstore specific fields"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Create the retriever\n","retriever = vectorstore.as_retriever()\n","configurable_retriever = retriever.configurable_fields(\n","    search_kwargs=ConfigurableField(\n","        id=\"search_kwargs\",\n","        name=\"Search Kwargs\",\n","        description=\"The search kwargs to use\",\n","    )\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We can now create the chain using our configurable retriever"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["chain = (\n","    {\"context\": configurable_retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Invoke the chain to answer a question from user 1"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["'Transformers are related to convolutional neural networks in that they both use convolutional neural networks as a basic building block. However, transformers differ from convolutional neural networks in that they rely entirely on an attention mechanism to draw global dependencies between input and output, while convolutional neural networks use convolutional operations to relate signals from different positions.'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke(\n","    \"How are transformers related to convolutional neural networks?\",\n","    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"user 1\"}}},\n",")"]},{"cell_type":"markdown","metadata":{},"source":["And now for user 2"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["'The datasets used to train Llama-2 include 2 trillion tokens of data from publicly available sources for pretraining, publicly available instruction datasets for fine-tuning, and over one million new human-annotated examples for fine-tuning.'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke(\n","    \"What datasets have benn used to train Llama-2?\",\n","    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"user 2\"}}},\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Now, we try to solve the question of user 2 using the user 1 "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["'Based on the given context, there is no information about the datasets used to train Llama-2.'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke(\n","    \"What datasets have benn used to train Llama-2?\",\n","    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"user 1\"}}},\n",")"]},{"cell_type":"markdown","metadata":{},"source":["As we expected, user 1 has no access to data of user 2, so it can not solve the question"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1lsT1V_U1Gq-jv09wv0ok5QHdyRjJyNxm","timestamp":1703241353984},{"file_id":"1llBvUXvy9sgvfOX-IaiAAHG9cZm7UC0A","timestamp":1696597847452},{"file_id":"1YXXWaUSFR_Km1NVL2best5hkPkh88Rhg","timestamp":1696115734680},{"file_id":"1J1FOyWE2dv9Pa_flwBdSsS2f7Ly6yllt","timestamp":1695946769549},{"file_id":"1sAmHXUlZNJg5ibJdTq7kQ1QLoR6Fy9fN","timestamp":1695946575577},{"file_id":"1_57iwYV205vVWrmJnfTUjWIo5XB3e-5g","timestamp":1695733772895},{"file_id":"1BT_kRFMP27lmwAoWeIhiie0VPs0oqShz","timestamp":1695731972211}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
