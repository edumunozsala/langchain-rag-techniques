{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense X Retrieval: Propositions as retrieval units for a RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dense retrieval has emerged as a crucial method for obtaining relevant context or knowledge in open-domain NLP tasks. However, the choice of the retrieval unit, i.e., the pieces of text in which the corpus is indexed, such as a document, passage, or sentence, is often overlooked when a learned dense retriever is applied to a retrieval corpus at inference time. The researchers found that the choice of retrieval unit significantly influences the performance of both retrieval and downstream tasks.\n",
    "\n",
    "This notebook, that joins the piecs of code of the Langcchain template, demonstrates the multi-vector indexing strategy proposed by Chen, et. al.'s [Dense X Retrieval: What Retrieval Granularity Should We Use?](https://arxiv.org/abs/2312.06648). The prompt directs an LLM to generate de-contextualized \"propositions\" which can be vectorized to increase the retrieval accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the enviroment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Propositional Chain to build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the prompt to extract the propositions as units for retieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from the paper to be more robust to benign prompt injection\n",
    "# https://arxiv.org/abs/2312.06648\n",
    "# @misc{chen2023dense,\n",
    "#       title={Dense X Retrieval: What Retrieval Granularity Should We Use?},\n",
    "#       author={Tong Chen and Hongwei Wang and Sihao Chen and Wenhao Yu and Kaixin Ma\n",
    "#               and Xinran Zhao and Hongming Zhang and Dong Yu},\n",
    "#       year={2023},\n",
    "#       eprint={2312.06648},\n",
    "#       archivePrefix={arXiv},\n",
    "#       primaryClass={cs.CL}\n",
    "# }\n",
    "PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Decompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of\n",
    "context.\n",
    "1. Split compound sentence into simple sentences. Maintain the original phrasing from the input\n",
    "whenever possible.\n",
    "2. For any named entity that is accompanied by additional descriptive information, separate this\n",
    "information into its own distinct proposition.\n",
    "3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences\n",
    "and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the\n",
    "entities they refer to.\n",
    "4. Present the results as a list of strings, formatted in JSON.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:\n",
    "The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in\n",
    "1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\n",
    "other parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were\n",
    "frequently seen in gardens in spring, and thus may have served as a convenient explanation for the\n",
    "origin of the colored eggs hidden there for children. Alternatively, there is a European tradition\n",
    "that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and\n",
    "both occur on grassland and are first seen in the spring. In the nineteenth century the influence\n",
    "of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.\n",
    "German immigrants then exported the custom to Britain and America where it evolved into the\n",
    "Easter Bunny.\"\n",
    "Output: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in\n",
    "1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of\n",
    "medicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until\n",
    "the 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about\n",
    "the possible explanation for the connection between hares and the tradition during Easter\", \"Hares\n",
    "were frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation\n",
    "for the origin of the colored eggs hidden in gardens for children.\", \"There is a European tradition\n",
    "that hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both\n",
    "hares and lapwing’s nests occur on grassland and are first seen in the spring.\", \"In the nineteenth\n",
    "century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular\n",
    "throughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to\n",
    "Britain and America.\", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\n",
    "Britain and America.\"]\"\"\",  # noqa\n",
    "        ),\n",
    "        (\"user\", \"Decompose the following:\\n{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the propositional chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propositions(tool_calls: list) -> list:\n",
    "    if not tool_calls:\n",
    "        raise ValueError(\"No tool calls found\")\n",
    "    return tool_calls[0][\"args\"][\"propositions\"]\n",
    "\n",
    "\n",
    "def empty_proposals(x):\n",
    "    # Model couldn't generate proposals\n",
    "    return []\n",
    "\n",
    "\n",
    "proposition_chain = (\n",
    "    PROMPT\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo-16k\").bind(\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"decompose_content\",\n",
    "                    \"description\": \"Return the decomposed propositions\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"propositions\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"propositions\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"decompose_content\"}},\n",
    "    )\n",
    "    | JsonOutputToolsParser()\n",
    "    | get_propositions\n",
    ").with_fallbacks([RunnableLambda(empty_proposals)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the components of the Main RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section build the components of the RAG chain and create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.load import load\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCSTORE_ID_KEY = \"doc_id\"\n",
    "DOCSTORE_DIR=\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the retriever for the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_vector_retriever(docstore_id_key: str, collection_name: str):\n",
    "    \"\"\"Create the composed retriever object.\"\"\"\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=str(Path(DOCSTORE_DIR) / \"chroma_db_proposals\"),\n",
    "        embedding_function=OpenAIEmbeddings(),\n",
    "    )\n",
    "    store = LocalFileStore(\n",
    "        str(Path(DOCSTORE_DIR) / \"multi_vector_retriever_metadata\")\n",
    "    )\n",
    "    return MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        byte_store=store,\n",
    "        id_key=docstore_id_key,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list) -> str:\n",
    "    loaded_docs = [load(doc) for doc in docs]\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            f\"<Document id={i}>\\n{doc.page_content}\\n</Document>\"\n",
    "            for i, doc in enumerate(loaded_docs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    The RAG chain\n",
    "\n",
    "    :param retriever: A function that retrieves the necessary context for the model.\n",
    "    :return: A chain of functions representing the multi-modal RAG process.\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\", max_tokens=1024)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an AI assistant. Answer based on the retrieved documents:\"\n",
    "                \"\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "            ),\n",
    "            (\"user\", \"{question}?\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define the RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Retriever and the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multi-vector retriever\n",
    "retriever = get_multi_vector_retriever(DOCSTORE_ID_KEY, \"attention-paper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain = rag_chain(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    __root__: str\n",
    "\n",
    "\n",
    "chain = chain.with_types(input_type=Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest the data, define the propositional retriever and build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "from typing import Sequence\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# For our example, we'll load docs from the web\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # noqa\n",
    "from langchain_community.document_loaders.recursive_url_loader import (\n",
    "        RecursiveUrlLoader,\n",
    "    )  # noqa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to build the index and include the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_documents(\n",
    "    retriever,\n",
    "    propositions: Sequence[Sequence[str]],\n",
    "    docs: Sequence[Document],\n",
    "    id_key: str = DOCSTORE_ID_KEY,\n",
    "):\n",
    "    doc_ids = [\n",
    "        str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.metadata[\"source\"])) for doc in docs\n",
    "    ]\n",
    "    prop_docs = [\n",
    "        Document(page_content=prop, metadata={id_key: doc_ids[i]})\n",
    "        for i, props in enumerate(propositions)\n",
    "        for prop in props\n",
    "        if prop\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(prop_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "\n",
    "def create_index(\n",
    "    docs: Sequence[Document],\n",
    "    indexer: Runnable,\n",
    "    docstore_id_key: str = DOCSTORE_ID_KEY,\n",
    "    collection_name: str = \"default\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes docs and their propositions\n",
    "\n",
    "    :param docs: Documents to index\n",
    "    :param indexer: Runnable creates additional propositions per doc\n",
    "    :param docstore_id_key: Key to use to store the docstore id\n",
    "    :return: Retriever\n",
    "    \"\"\"\n",
    "    print(\"Creating multi-vector retriever\")\n",
    "    retriever = get_multi_vector_retriever(docstore_id_key, collection_name)\n",
    "    propositions = indexer.batch(\n",
    "        [{\"input\": doc.page_content} for doc in docs], {\"max_concurrency\": 10}\n",
    "    )\n",
    "\n",
    "    add_documents(\n",
    "        retriever,\n",
    "        propositions,\n",
    "        docs,\n",
    "        id_key=docstore_id_key,\n",
    "    )\n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the index using the proposiotional chain to define decomposed propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n",
      "Split into 7 documents\n",
      "Creating multi-vector retriever\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Could add more parsing here, as it's very raw.\n",
    "loader = RecursiveUrlLoader(\n",
    "        \"https://ar5iv.labs.arxiv.org/html/1706.03762\",\n",
    "        max_depth=2,\n",
    "        extractor=lambda x: Soup(x, \"html.parser\").text,\n",
    "    )\n",
    "data = loader.load()\n",
    "print(f\"Loaded {len(data)} documents\")\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(f\"Split into {len(all_splits)} documents\")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_index(\n",
    "        all_splits,\n",
    "        proposition_chain,\n",
    "        DOCSTORE_ID_KEY,\n",
    "        \"llama2-paper\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers and Convolutional Neural Networks (CNNs) are both types of neural network architectures, but they are designed for different purposes and operate on different principles.\\n\\nConvolutional Neural Networks (CNNs) are designed primarily for processing data that has a known grid-like topology, such as image data. They are characterized by their use of convolutional layers, which apply a convolution operation to the input to capture the local dependencies and the spatial hierarchy in the data. This makes them particularly well-suited for tasks like image recognition, as they can efficiently process the pixel data and learn features like edges, textures, and shapes.\\n\\nTransformers, on the other hand, were introduced by Vaswani et al. in the paper \"Attention Is All You Need\" (2017) and are designed to handle sequential data, such as text for natural language processing tasks. The key innovation in transformers is the attention mechanism, which allows the model to weigh the influence of different parts of the input data differently. This is particularly useful for tasks like language translation, where the relevance of a word can depend on the context provided by the rest of the sentence.\\n\\nWhile CNNs use the convolution operation to capture local dependencies, transformers use self-attention to capture global dependencies, meaning that they can consider the entire context of the input sequence at once. This allows transformers to handle long-range dependencies and variable-length input sequences effectively.\\n\\nHowever, there is a connection between the two in the form of the work by Gehring et al. (2017) mentioned in the provided documents, which explores the idea of \"Convolutional Sequence to Sequence Learning.\" In this work, the authors propose a model that combines aspects of both CNNs and sequence-to-sequence learning (which is the domain where transformers are typically applied). This model uses convolutional layers to process sequential data, aiming to capture the benefits of both CNNs (efficient processing of local patterns) and the sequence-to-sequence approach (handling of variable-length sequences and long-range dependencies).\\n\\nIn summary, while CNNs and transformers are distinct architectures with different primary applications, there is research exploring the intersection of the two, aiming to leverage the strengths of both in hybrid models.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"How are transformers related to convolutional neural networks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
